<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Cache-Control" content="public, max-age=86400">
    <meta http-equiv="Pragma" content="cache">
    <meta http-equiv="Expires" content="86400">
    <title>Emergency Display</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        img {
            width: 700px;
            height: auto;
            display: block;
        }
    </style>
</head>
<body>
    <img src="emergency.jpg" alt="Emergency Image" onload="preloadAudio()">
    <script>
        // グローバルなAudioContextとAudioBuffer
        let audioContext = null;
        let audioBuffer = null;

        // 音声の事前読み込みとデコード
        function preloadAudio() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
            const preloadFetch = new Request('ring.mp3', {
                method: 'GET',
                cache: 'force-cache'
            });
            fetch(preloadFetch)
                .then(response => {
                    if (!response.ok) throw new Error('Audio fetch failed');
                    return response.arrayBuffer();
                })
                .then(data => audioContext.decodeAudioData(data))
                .then(buffer => {
                    audioBuffer = buffer; // グローバルに保存
                })
                .catch(() => {
                    // キャッシュから再試行
                    const cacheFetch = new Request('ring.mp3', {
                        method: 'GET',
                        cache: 'only-if-cached',
                        mode: 'same-origin'
                    });
                    fetch(cacheFetch)
                        .then(response => response.arrayBuffer())
                        .then(data => audioContext.decodeAudioData(data))
                        .then(buffer => {
                            audioBuffer = buffer;
                        })
                        .catch(() => {});
                });
        }

        // 現在時刻を取得
        const now = new Date();
        const hours = now.getHours();
        const minutes = now.getMinutes();

        // 対象時刻（時:分）のリスト
        const targetTimes = [
            { hour: 9, minute: 0 },
            { hour: 12, minute: 0 },
            { hour: 15, minute: 0 },
            { hour: 18, minute: 0 },
            { hour: 21, minute: 0 }
        ];

        // 現在時刻が対象時刻と一致するかチェック
        const isTargetTime = targetTimes.some(time => 
            hours === time.hour && minutes === time.minute
        );

        // 対象時刻の場合に音声を再生
        if (isTargetTime && audioContext && audioBuffer) {
            // AudioContextがsuspendedの場合、resume
            if (audioContext.state === 'suspended') {
                audioContext.resume().then(() => playAudio());
            } else {
                playAudio();
            }
        }

        function playAudio() {
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            source.start(0);
            // 再生終了後にリソース解放
            source.onended = () => {
                source.disconnect();
                source.buffer = null; // 参照を解放
            };
        }
    </script>
</body>
</html>